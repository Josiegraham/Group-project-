{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02625d80",
   "metadata": {},
   "source": [
    "# Data cleaning script "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf8614",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "851ee43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\josie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\josie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import requests\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfe4d53",
   "metadata": {},
   "source": [
    "### Upload csv file\n",
    "We are using data from 2 sources; Nexus and Kaggle. The Nexus data is already fairly clean as it allowed us to choose which columns we required and filter the data before downloading it to a csv. \n",
    "\n",
    "The Kaggle data requires a bit more cleaning and reorganising before we are able to concatonate the data sets together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "78b43ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload Nexus csv \n",
    "Nexus_csv_path = 'C:/Users/josie/OneDrive/Documents/UWE/Group project/Headline_data_v2.csv'\n",
    "Nexus_data_raw = pd.read_csv(csv_path, encoding='windows-1252')\n",
    "\n",
    "\n",
    "#Upload Kaggle csv\n",
    "Kaggle_csv_path = 'C:/Users/josie/OneDrive/Documents/UWE/Group project/Kaggle data.csv'\n",
    "Kaggle_data_raw = pd.read_csv(Kaggle_csv_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36321599",
   "metadata": {},
   "source": [
    "## Restructuring Kaggle data \n",
    "The Kaggle data has extra information on the price of Bitcoin. This information is not given in the Nexus data set and to keep the consistency it is better to get the Bitcoin price information from the same source to cover teh entire data set. This means these collumns can be dropped. The symbol column can also be dropped the whole column is 'BTC' as an entry which doesn't provide much insite. \n",
    "\n",
    "The kaggle data has multiple headlines per date. This needs to be restructured to 1 headline per row.\n",
    "\n",
    "The collumn also need to be renamed to be consistent with the Nexus data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "39d28c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns \n",
    "Kaggle_data = Kaggle_data_raw.drop(columns=['Unnamed: 0', 'open_price', 'close_price','high_price', 'low_price', 'symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5cd0b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separates each headline onto a new row.\n",
    "Kaggle_data['articles'] = Kaggle_data['articles'].apply(lambda x: x.split(\"', \"))\n",
    "Kaggle_data = Kaggle_data.explode('articles').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0c5525bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename collumns in order to combine with Nexus data\n",
    "Kaggle_data = Kaggle_data.rename(columns={'begins_at': 'Published date', 'articles': 'Headline'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45fad52",
   "metadata": {},
   "source": [
    "## Removing non-English headlines\n",
    "When downloading the Nexus data we were able to pick parameters for the data we wanted, this meant we could select only English language papers. For the Kaggle data there is a mixture of different languages. As we are only focusing on English language papers these need to be removed. Dectecting language takes a lot of computing power so it is important to do this step before combining with the Nexus data. \n",
    "\n",
    "To remove them, a new column is created which has the detected language in it. If the language is not detected it is set to 'unknown', this prevents error messages. Then only the 'en' language papers are kept in the data frame and the lang collumn is dropped as it is no longer needed. \n",
    "\n",
    "Originally when running the code there was a SettingWithCopyWarning. To avoid this warning .loc has been used to access the lang column. For further information on SettingWithCopyWarning 'https://www.analyticsvidhya.com/blog/2021/11/3-ways-to-deal-with-settingwithcopywarning-in-pandas/#:~:text='SettingWithCopy'%20is%20a%20common%20warning,settingwithcopywarning%20new%20column%20pops%20up'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c3571b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Published date</th>\n",
       "      <th>Headline</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25/02/2018</td>\n",
       "      <td>['Original Pizza Day Purchaser Does It Again With Bitcoin Lightning Network</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25/02/2018</td>\n",
       "      <td>'This 11-year-old just wrote a book on bitcoin that hopefully a kid can \\nunderstand</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25/02/2018</td>\n",
       "      <td>'Without Mentioning Blockchain, Putin Says That Russia Must Stay Ahead In \\nTechnology</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25/02/2018</td>\n",
       "      <td>'El comprador original del Pizza Day lo hace de nuevo con la Lightning \\nNetwork de Bitcoin</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25/02/2018</td>\n",
       "      <td>'Meet the strippers tattooed with BARCODES so sneaky punters can tip in \\nBitcoin without their partner...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Published date  \\\n",
       "0     25/02/2018   \n",
       "1     25/02/2018   \n",
       "2     25/02/2018   \n",
       "3     25/02/2018   \n",
       "4     25/02/2018   \n",
       "\n",
       "                                                                                                     Headline  \\\n",
       "0                                 ['Original Pizza Day Purchaser Does It Again With Bitcoin Lightning Network   \n",
       "1                        'This 11-year-old just wrote a book on bitcoin that hopefully a kid can \\nunderstand   \n",
       "2                      'Without Mentioning Blockchain, Putin Says That Russia Must Stay Ahead In \\nTechnology   \n",
       "3                 'El comprador original del Pizza Day lo hace de nuevo con la Lightning \\nNetwork de Bitcoin   \n",
       "4  'Meet the strippers tattooed with BARCODES so sneaky punters can tip in \\nBitcoin without their partner...   \n",
       "\n",
       "  lang  \n",
       "0   en  \n",
       "1   en  \n",
       "2   en  \n",
       "3   es  \n",
       "4   en  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dectect language of newspaper \n",
    "from langdetect import detect\n",
    "\n",
    "# Safe detect for when the language is not detecable\n",
    "def safe_detect(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "# This function takes a long time to perform\n",
    "# Use .loc to set the new column to avoid SettingWithCopyWarning\n",
    "Kaggle_data.loc[:, 'lang'] = Kaggle_data['Headline'].apply(lambda x: safe_detect(x))\n",
    "Kaggle_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "11010ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English headlines\n",
    "Kaggle_data = Kaggle_data[Kaggle_data['lang'] == 'en']\n",
    "Kaggle_data = Kaggle_data.drop('lang', axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91416474",
   "metadata": {},
   "source": [
    "## Combining data into a single data frame.\n",
    "The Kaggle data has been reshaped and renamed so that is is now compatible to combine with the Nexus data. The Kaggle data only contains the publish date and the headline, whereas the nexus data has 2 other columns. This will just mean that for Kaggle data there will be an NA for entries in these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e6d182cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19363\n",
      "19363\n"
     ]
    }
   ],
   "source": [
    "headline_data = pd.concat([Nexus_data_raw, Kaggle_data])\n",
    "\n",
    "#Checks\n",
    "print(len(Nexus_data_raw) + len(Kaggle_data))\n",
    "print(len(headline_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d84c4",
   "metadata": {},
   "source": [
    "## Cleaning data frame.\n",
    "The duplicate rows are removed as this could alter the senitment analysis if there are multiple entries for one headline. The null headlines are also removevd as they provide no extra information. \n",
    "\n",
    "The data then needs to be cleaned specifically for sentiment analysis. Headlings are lower cased and unctuation removed to reduce noise and so that the model can focus on the more meaningful parts of the headline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a848d",
   "metadata": {},
   "source": [
    "#### Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b7b5b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dulpicate rows\n",
    "headline_data.drop_duplicates(subset=['Headline'], keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d40d64e",
   "metadata": {},
   "source": [
    "#### Remove Null headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e7fbad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes all missing headlines\n",
    "headline_data.dropna(subset=['Headline'])\n",
    "\n",
    "headline_data['Headline'] = headline_data['Headline'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fac946",
   "metadata": {},
   "source": [
    "#### Lower case headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "60fd085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case headlines\n",
    "# Loops through every word in the the Title collumn of the data frame and applies the lower() function\n",
    "# The lower function only works on strings, so the Title collumn must be split up first and then rejoined again after. \n",
    "headline_data['Cleaned Headline'] = headline_data['Headline'].apply(lambda x: ' '.join(word.lower() for word in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691fc22",
   "metadata": {},
   "source": [
    "#### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4a4f5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "\n",
    "# Strip \\n \n",
    "# This symbol indicates a new line. As it involves a punctuation symbol, this step must be done before removing punctuation.\n",
    "headline_data['Cleaned Headline'] = headline_data['Cleaned Headline'].str.replace(r'\\n', '')\n",
    "\n",
    "# First replacing the hyphen with a space. If the hyphen is just removed it creates combined words like bitcoinbased.\n",
    "# The regex means that anything that isn't a word or a space is removed. \n",
    "headline_data['Cleaned Headline'] = headline_data['Cleaned Headline'].str.replace('-', ' ').replace(r'[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b762f177",
   "metadata": {},
   "source": [
    "### Remove stop words\n",
    "Stop words are a set of commonly used words in a language. In the context of NLP, they carry very little meaning and so are removed to further reduce noise. \n",
    "\n",
    "The library of stop words from the nltk module is used to initially remove stop words. Then there is a count of most commonly occuring words in the data set. From there more specific to crypto stop words can be picked out and removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "30b55885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "\n",
    "# Imported list of English stop words for NLTK\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Loops through and if word not in the stop words list then it is kept in the headline. (all words in stop word list removed)\n",
    "headline_data['Cleaned Headline'] = headline_data['Cleaned Headline'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73676e60",
   "metadata": {},
   "source": [
    "### Dectecting more specific stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a56f3095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bitcoin           12275\n",
       "btc                2620\n",
       "crypto             2305\n",
       "price              2031\n",
       "new                 792\n",
       "ethereum            762\n",
       "bitcoins            748\n",
       "says                723\n",
       "market              708\n",
       "cryptocurrency      654\n",
       "mining              650\n",
       "bitcoinistcom       626\n",
       "analysis            552\n",
       "us                  511\n",
       "could               475\n",
       "million             430\n",
       "time                404\n",
       "high                390\n",
       "first               383\n",
       "year                360\n",
       "trading             352\n",
       "blockchain          346\n",
       "eth                 344\n",
       "buy                 329\n",
       "exchange            317\n",
       "money               312\n",
       "heres               307\n",
       "gold                298\n",
       "markets             296\n",
       "investors           293\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our own list of stop words. The imported stop words are a good start but not specific to crypto headline data.\n",
    "\n",
    "# Check frequency of words.\n",
    "# Puts all the words for headlines into 1 long sentence, then it is put into a series with the value_count function.\n",
    "# This gives a count of each words that comes up and we can manually check for words that don't add meaning to remove.\n",
    "# This step will need to be repeated everytime a new data set of headlines is added.\n",
    "pd.Series(\" \".join(headline_data['Cleaned Headline']).split()).value_counts()[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608e8da",
   "metadata": {},
   "source": [
    "### Selecting specific stop words to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5a4f4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is very subjective. \n",
    "# It is just deciding which words aren't as relavent to help reduce noise in the sentiment analysis\n",
    "manual_stop_words = ['us', 'back', 'says', 'sam', 'sec', 'could', 'bankmanfried', 'man', 'wall', 'woman', 'briefing', 'amid',\n",
    "                    'get', 'lucy', 'know']\n",
    "headline_data['Cleaned Headline'] = headline_data['Cleaned Headline'].apply(lambda x: ' '.join(word for word in x.split() if word not in manual_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4abfe58",
   "metadata": {},
   "source": [
    "### Changing abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e7bacb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the word freq I noticed 5bn. I am going to separate numbers and replace bn with billion and mn with million.\n",
    "headline_data['Cleaned Headline'] = headline_data['Cleaned Headline'].str.replace(r\"bn\", r\" billion\" , regex=True).replace(r\"mn\", r\" million\" , regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba345a88",
   "metadata": {},
   "source": [
    "## Headline relevance checker\n",
    "Due to the sheer number of headlines it is too time consuming to go through every one to decide if they are relavent to bitcoin or not. To get around this a new column is added to the data frame that has the number of crypto relavent words in the headline. \n",
    "\n",
    "There were no suitable pre made list of crypto relavent words online. This mean we have to scrap the website 'https://cryptonest.co.uk/pages/crypto-dictionary' for all their crypto definition using beautiful soup. All the definition words were within the class 'page-content page-content--medium rte' and written in bold so it was easy to locate and isolate only the key words. \n",
    "\n",
    "Challenges arose as some key words had their corresponing abbreviation in brackets e.g Ether (ETH). Some keywords had a slash in them to indicate similar words e.g. bear / bearish. This meant that the string to compare to the headline was with key word and abbreviaiton. These needed to be separated out into two different key words so that they key word and abbreviaiton could be compared separately. \n",
    "\n",
    "A set was used to make sure there were no duplicate words in the keywords. This was then converted back into a list as sets are immutable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f5a8c6",
   "metadata": {},
   "source": [
    "#### Scraping website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6dfa0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Website with crypto dictionary\n",
    "url = 'https://cryptonest.co.uk/pages/crypto-dictionary' \n",
    "\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the content with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Class containing all the definitions in\n",
    "class_name = \"page-content page-content--medium rte\" \n",
    "\n",
    "# Find all elements with the specified class\n",
    "elements_with_class = soup.find_all(class_=class_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77776c3f",
   "metadata": {},
   "source": [
    "#### Cleaning Key words to be compatible with headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8f1305b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to remove all punctuation except brackets\n",
    "\n",
    "# Initialize a set to store cleaned words\n",
    "crypto_words_set = set()\n",
    "\n",
    "# Regular expression to find text within brackets\n",
    "brackets_pattern = re.compile(r'\\(([^)]+)\\)')\n",
    "\n",
    "# Iterate over elements and find all <strong> tags within each\n",
    "for element in elements_with_class:\n",
    "    strong_tags = element.find_all('strong')\n",
    "    for key_words in strong_tags:\n",
    "        # Get the text and convert to lowercase\n",
    "        text = key_words.get_text().lower()\n",
    "        \n",
    "        # Remove hyphens \n",
    "        text = text.replace('-', ' ')\n",
    "        \n",
    "        # Extract bracketed words\n",
    "        bracketed_words = brackets_pattern.findall(text)\n",
    "        \n",
    "        # Remove bracketed words from the original text\n",
    "        text_without_brackets = re.sub(brackets_pattern, '', text)\n",
    "        \n",
    "        # Process original text without bracketed parts\n",
    "        split_words = text_without_brackets.split('/')\n",
    "        \n",
    "        # Add non-bracketed words to the set\n",
    "        for word in split_words:\n",
    "            cleaned_word = word.strip()\n",
    "            if cleaned_word:\n",
    "                crypto_words_set.add(cleaned_word)\n",
    "        \n",
    "        # Add bracketed words separately to the set\n",
    "        for word in bracketed_words:\n",
    "            cleaned_word = word.strip()\n",
    "            if cleaned_word:\n",
    "                crypto_words_set.add(cleaned_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aae628",
   "metadata": {},
   "source": [
    "#### Adding missed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "00b6873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From scanning the list crypto and bitcoins werent included so manually adding it to the set.\n",
    "# Bitcoins doesn't lemmatise to bitcoin I tihnk this is because it is specific to crypto.\n",
    "crypto_words_set.add('bitcoins')\n",
    "\n",
    "# Convert to a list so it can be used in function below\n",
    "crypto_words = list(crypto_words_set)\n",
    "\n",
    "# print(crypto_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a4da2",
   "metadata": {},
   "source": [
    "#### Adding count collumn for number of crypto relavent word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0b2049d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some words contain crypto e.g. cryptoqueen were not being picked up. This ensures all variations are counted\n",
    "containing_crypto = re.compile(r'\\b\\w*crypto\\w*\\b')\n",
    "containing_coin = re.compile(r'\\b\\w*coin\\w*\\b')\n",
    "\n",
    "# Apply the function to calculate the count\n",
    "headline_data['Count'] = headline_data['Cleaned Headline'].apply(\n",
    "    lambda x: sum(1 for word in x.split() if containing_crypto.search(word) or containing_coin.search(word) or word in crypto_words)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3c5e6",
   "metadata": {},
   "source": [
    "#### Selecting only the relavent headlines.\n",
    "A csv is made of both relavent and non-relavent so they can be skimmed through to double check if the are relavent or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f6613878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Published date</th>\n",
       "      <th>Countries</th>\n",
       "      <th>Company</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Cleaned Headline</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30/05/2024</td>\n",
       "      <td>England &amp; Wales</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Woman jailed for Bitcoin laundering</td>\n",
       "      <td>jailed bitcoin laundering</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06/06/2024</td>\n",
       "      <td>England &amp; Wales</td>\n",
       "      <td>BEST INC</td>\n",
       "      <td>Best Crypto Casino Sites &amp; Bitcoin Casinos in the UK</td>\n",
       "      <td>best crypto casino sites bitcoin casinos uk</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21/05/2024</td>\n",
       "      <td>United Kingdom of Great Britain and Northern Ireland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wright lied extensively as 'bitcoin inventor'</td>\n",
       "      <td>wright lied extensively bitcoin inventor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26/05/2024</td>\n",
       "      <td>United Kingdom of Great Britain and Northern Ireland</td>\n",
       "      <td>METROPOLITAN BANK HOLDING CORP</td>\n",
       "      <td>Ex-takeaway worker with Bitcoin worth more than £2bn jailed for six years</td>\n",
       "      <td>ex takeaway worker bitcoin worth 2 billion jailed six years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21/05/2024</td>\n",
       "      <td>United Kingdom of Great Britain and Northern Ireland</td>\n",
       "      <td>BIRD &amp; BIRD LLP</td>\n",
       "      <td>Inventor of bitcoin' given court rebuke</td>\n",
       "      <td>inventor bitcoin given court rebuke</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Published date                                             Countries  \\\n",
       "0     30/05/2024                                       England & Wales   \n",
       "1     06/06/2024                                       England & Wales   \n",
       "2     21/05/2024  United Kingdom of Great Britain and Northern Ireland   \n",
       "3     26/05/2024  United Kingdom of Great Britain and Northern Ireland   \n",
       "4     21/05/2024  United Kingdom of Great Britain and Northern Ireland   \n",
       "\n",
       "                          Company  \\\n",
       "0                             NaN   \n",
       "1                        BEST INC   \n",
       "2                             NaN   \n",
       "3  METROPOLITAN BANK HOLDING CORP   \n",
       "4                 BIRD & BIRD LLP   \n",
       "\n",
       "                                                                    Headline  \\\n",
       "0                                        Woman jailed for Bitcoin laundering   \n",
       "1                       Best Crypto Casino Sites & Bitcoin Casinos in the UK   \n",
       "2                              Wright lied extensively as 'bitcoin inventor'   \n",
       "3  Ex-takeaway worker with Bitcoin worth more than £2bn jailed for six years   \n",
       "4                                    Inventor of bitcoin' given court rebuke   \n",
       "\n",
       "                                              Cleaned Headline  Count  \n",
       "0                                    jailed bitcoin laundering      1  \n",
       "1                  best crypto casino sites bitcoin casinos uk      2  \n",
       "2                     wright lied extensively bitcoin inventor      1  \n",
       "3  ex takeaway worker bitcoin worth 2 billion jailed six years      1  \n",
       "4                          inventor bitcoin given court rebuke      1  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a csv of all the non-relavent headlines which can be double checked to make sure they are not relavent. \n",
    "countdf = headline_data[headline_data['Count'] == 0]\n",
    "countdf.to_csv('Count_0.csv')\n",
    "\n",
    "# Create a csv of all the relavent headlines to double check they are relavent. \n",
    "crypto_headline_data = headline_data[headline_data['Count'] != 0]\n",
    "cryptodf.to_csv('Crypto.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76413e6b",
   "metadata": {},
   "source": [
    "### Lemmatising words\n",
    "Lemmatising is the process to reverting words back to their root. This process is requires a lot of computing power so best to do it at the end when there is only the relavent headlines in their cleanest form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "30258b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josie\\AppData\\Local\\Temp\\ipykernel_31280\\4240550443.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  crypto_headline_data['Lem Headline'] = crypto_headline_data['Cleaned Headline']\\\n"
     ]
    }
   ],
   "source": [
    "# Import textblob\n",
    "from textblob import Word\n",
    "\n",
    "# Lemmatize final review format\n",
    "crypto_headline_data['Lem Headline'] = crypto_headline_data['Cleaned Headline']\\\n",
    ".apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "crypto_headline_data.to_csv('crypto_headline_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "77b903e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15585"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e589dea",
   "metadata": {},
   "source": [
    "### Converting American spelling to British (on hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "76708e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Code from https://stackoverflow.com/questions/42329766/python-nlp-british-english-vs-american-english\n",
    "# def convert_english(string):\n",
    "#     url =\"https://raw.githubusercontent.com/hyperreality/American-British-English-Translator/master/data/british_spellings.json\"\n",
    "#     british_to_american_dict = requests.get(url).json()    \n",
    "\n",
    "#     for british_spelling, american_spelling in british_to_american_dict.items():\n",
    "#         string = string.replace(american_spelling, british_spelling)\n",
    "  \n",
    "#     return string\n",
    "\n",
    "# headline_data['Cleaned Headline'] = headline_data['Cleaned Headline'].apply(lambda x: ' '.join(convert_english(word) for word in x.split()))\n",
    "# headline_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df8f335",
   "metadata": {},
   "source": [
    "## Notes to self of what I need to do\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d1143",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
